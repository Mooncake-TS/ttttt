# =========================================
# Streamlit LSTM ê°ì •ë¶„ë¥˜ ì•± (ë¬´í•œë¡œë”© ë°©ì§€)
# - Review.xlsx (Sentiment, Review)
# - í•™ìŠµì€ ë²„íŠ¼ìœ¼ë¡œ ì‹¤í–‰ + ìºì‹±í•´ì„œ 1íšŒë§Œ
# =========================================

import re
import numpy as np
import pandas as pd
import streamlit as st
from pathlib import Path

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers, models

st.set_page_config(page_title="ë¦¬ë·° ê°ì • ë¶„ì„", layout="centered")
st.title("ğŸ“Š ë¦¬ë·° ê°ì • ë¶„ì„ (LSTM)")
st.caption("Positive / Negative (2-class)")

# ----------------------------
# 0) ê²½ë¡œ/ë°ì´í„° ë¡œë“œ
# ----------------------------
BASE_DIR = Path(__file__).resolve().parent
DATA_PATH = BASE_DIR / "Review.xlsx"

if not DATA_PATH.exists():
    st.error("âŒ Review.xlsxë¥¼ ë ˆí¬ ë£¨íŠ¸(app.py ì˜†)ì— ì˜¬ë ¤ì¤˜ì•¼ í•´ìš”.")
    st.stop()

df = pd.read_excel(DATA_PATH)

required_cols = {"Sentiment", "Review"}
if not required_cols.issubset(df.columns):
    st.error(f"âŒ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {required_cols} / í˜„ì¬: {list(df.columns)}")
    st.stop()

df = df[["Sentiment", "Review"]].dropna().copy()
df["Sentiment"] = df["Sentiment"].astype(str)
df["Review"] = df["Review"].astype(str)

st.success(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)} rows")
st.write("ë¼ë²¨ ë¶„í¬:", df["Sentiment"].value_counts().to_dict())

# ----------------------------
# 1) ì „ì²˜ë¦¬/ë¶ˆìš©ì–´
# ----------------------------
stopwords = set([
    "ì´", "ê°€", "ì„", "ë¥¼", "ì€", "ëŠ”", "ì—", "ì—ì„œ", "ì—ê²Œ",
    "ì˜", "ì™€", "ê³¼", "ë„", "ë¡œ", "ìœ¼ë¡œ",
    "í•˜ë‹¤", "ë˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤",
    "ê·¸", "ì €", "ê²ƒ", "ìˆ˜",
    "ì¢€", "ì˜", "ë§¤ìš°", "ì •ë§", "ë„ˆë¬´",
    "ë•Œë¬¸", "ê°™ë‹¤"
])

def clean_text(text: str) -> str:
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    tokens = text.split()
    tokens = [t for t in tokens if t not in stopwords and len(t) > 1]
    return " ".join(tokens)

# ----------------------------
# 2) í•™ìŠµ í•¨ìˆ˜ (ìºì‹±: í•œ ë²ˆë§Œ í•™ìŠµ)
# ----------------------------
@st.cache_resource(show_spinner=False)
def train_once(df_in: pd.DataFrame, vocab_size=15000, max_len=120, epochs=4):
    # ì¤€ë¹„
    texts = [clean_text(t) for t in df_in["Review"].tolist()]
    labels = df_in["Sentiment"].tolist()

    le = LabelEncoder()
    y = le.fit_transform(labels)

    X_train, X_test, y_train, y_test = train_test_split(
        texts, y, test_size=0.2, random_state=42, stratify=y
    )

    tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
    tokenizer.fit_on_texts(X_train)

    def encode(texts_list):
        seq = tokenizer.texts_to_sequences(texts_list)
        return pad_sequences(seq, maxlen=max_len, padding="post", truncating="post")

    X_train_pad = encode(X_train)
    X_test_pad = encode(X_test)

    # ëª¨ë¸
    model = models.Sequential([
        layers.Input(shape=(max_len,)),
        layers.Embedding(vocab_size, 128, mask_zero=True),
        layers.Bidirectional(layers.LSTM(64)),
        layers.Dropout(0.3),
        layers.Dense(32, activation="relu"),
        layers.Dense(len(le.classes_), activation="softmax")
    ])
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

    # í•™ìŠµ (ë„ˆë¬´ ê¸¸ë©´ cloudì—ì„œ ë‹µë‹µí•´ì ¸ì„œ epochs ë‚®ê²Œ)
    model.fit(
        X_train_pad, y_train,
        validation_split=0.1,
        epochs=epochs,
        batch_size=16,
        verbose=0
    )

    loss, acc = model.evaluate(X_test_pad, y_test, verbose=0)

    return model, tokenizer, le, (loss, acc), max_len

# ----------------------------
# 3) í•™ìŠµ ë²„íŠ¼ (ì—¬ê¸°ì„œë§Œ í•™ìŠµ ì‹œì‘)
# ----------------------------
st.subheader("1) ëª¨ë¸ í•™ìŠµ")

col1, col2, col3 = st.columns(3)
with col1:
    EPOCHS = st.slider("epochs", 1, 10, 4)
with col2:
    VOCAB_SIZE = st.selectbox("vocab_size", [5000, 10000, 15000, 20000], index=2)
with col3:
    MAX_LEN = st.selectbox("max_len", [80, 100, 120, 150], index=2)

train_btn = st.button("ğŸ§  í•™ìŠµí•˜ê¸° (1íšŒë§Œ)")

if train_btn:
    with st.spinner("í•™ìŠµ ì¤‘... (í•œ ë²ˆë§Œ ëŒê³  ìºì‹œì— ì €ì¥ë©ë‹ˆë‹¤)"):
        model, tokenizer, le, metrics, max_len = train_once(
            df, vocab_size=VOCAB_SIZE, max_len=MAX_LEN, epochs=EPOCHS
        )
    st.session_state["model"] = model
    st.session_state["tokenizer"] = tokenizer
    st.session_state["le"] = le
    st.session_state["max_len"] = max_len
    st.session_state["metrics"] = metrics

# ì´ë¯¸ í•™ìŠµëœ ìºì‹œê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ì²« ì‹¤í–‰ í›„ë¶€í„°)
if "model" not in st.session_state:
    st.info("ì•„ì§ í•™ìŠµ ì „ì…ë‹ˆë‹¤. ìœ„ì˜ **í•™ìŠµí•˜ê¸°** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
else:
    loss, acc = st.session_state["metrics"]
    st.success(f"âœ… í•™ìŠµ ì™„ë£Œ (Test Acc={acc:.3f}, Loss={loss:.3f})")
    st.write("ë¼ë²¨ ë§¤í•‘:", dict(zip(st.session_state["le"].classes_, range(len(st.session_state["le"].classes_)))))

# ----------------------------
# 4) ì…ë ¥ ì˜ˆì¸¡
# ----------------------------
st
