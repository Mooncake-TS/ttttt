# app.py
# =========================================
# Streamlit - LSTM ê°ì •ë¶„ë¥˜ (Review.xlsx ê¸°ë°˜)
# - ì…ë ¥: Review.xlsx (columns: Sentiment, Review)
# - í•™ìŠµ: Tokenizer + Embedding + BiLSTM
# - ê¸°ëŠ¥: (1) í•™ìŠµ (2) ì„±ëŠ¥ í™•ì¸ (3) ë¦¬ë·° ì…ë ¥ -> ì˜ˆì¸¡
# =========================================

import re
import numpy as np
import pandas as pd
import streamlit as st

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers, models

import matplotlib.pyplot as plt

st.set_page_config(page_title="ë¦¬ë·° ê°ì •ë¶„ì„ (LSTM)", layout="wide")

# ----------------------------
# 0) ê¸°ë³¸ ì„¤ì •
# ----------------------------
DEFAULT_XLSX_PATH = "data/Review.xlsx"  # í”„ë¡œì íŠ¸ í´ë” ì•ˆì— data/Review.xlsx ë„£ìœ¼ë©´ ìë™ ì¸ì‹

# ê°ë…ê´€ ìš”êµ¬ ë¶ˆìš©ì–´(ì˜ˆì‹œ) + "ë„ˆë¬´" ì¶”ê°€
STOPWORDS = set([
    "ì´", "ê°€", "ì„", "ë¥¼", "ì€", "ëŠ”", "ì—", "ì—ì„œ", "ì—ê²Œ",
    "ì˜", "ì™€", "ê³¼", "ë„", "ë¡œ", "ìœ¼ë¡œ",
    "í•˜ë‹¤", "ë˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤",
    "ê·¸", "ì €", "ê²ƒ", "ìˆ˜",
    "ì¢€", "ì˜", "ë§¤ìš°", "ì •ë§",
    "ë•Œë¬¸", "ê°™ë‹¤",
    "ë„ˆë¬´"
])

def clean_text(s: str) -> str:
    s = str(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def simple_tokenize(s: str):
    """
    konlpy ì—†ì´ë„ ëŒì•„ê°€ê²Œ 'ê°€ë²¼ìš´ í† í°í™”' ë²„ì „.
    - í•œê¸€/ì˜ë¬¸/ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°
    - ê³µë°± split
    - ë¶ˆìš©ì–´ ì œê±°
    """
    s = clean_text(s)
    s = re.sub(r"[^0-9a-zA-Zê°€-í£\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    tokens = s.split()
    tokens = [t for t in tokens if (t not in STOPWORDS and len(t) > 1)]
    return tokens

def preprocess_to_string(s: str) -> str:
    # TokenizerëŠ” ë¬¸ìì—´ì„ ë°›ìœ¼ë‹ˆê¹Œ, í† í°ë“¤ì„ ê³µë°±ìœ¼ë¡œ ë‹¤ì‹œ join
    return " ".join(simple_tokenize(s))

def load_review_xlsx(uploaded_file=None, fallback_path=DEFAULT_XLSX_PATH):
    if uploaded_file is not None:
        df = pd.read_excel(uploaded_file)
        return df
    # ì—…ë¡œë“œ ì•ˆ í–ˆìœ¼ë©´ ë¡œì»¬ íŒŒì¼(í”„ë¡œì íŠ¸ ë‚´ data/Review.xlsx) ì‹œë„
    try:
        df = pd.read_excel(fallback_path)
        return df
    except Exception:
        return None

def build_model(vocab_size: int, max_len: int, num_classes: int,
                emb_dim=128, lstm_units=64, dropout=0.3):
    model = models.Sequential([
        layers.Input(shape=(max_len,)),
        layers.Embedding(input_dim=vocab_size, output_dim=emb_dim, mask_zero=True),
        layers.Bidirectional(layers.LSTM(lstm_units)),
        layers.Dropout(dropout),
        layers.Dense(64, activation="relu"),
        layers.Dropout(dropout),
        layers.Dense(num_classes, activation="softmax")
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model

def plot_confusion(cm, class_names):
    fig = plt.figure()
    plt.imshow(cm)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.xticks(range(len(class_names)), class_names, rotation=45, ha="right")
    plt.yticks(range(len(class_names)), class_names)
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, str(cm[i, j]), ha="center", va="center")
    plt.tight_layout()
    return fig

# ----------------------------
# UI
# ----------------------------
st.title("ë¦¬ë·° ê°ì • ë¶„ì„ (LSTM) â€” Positive / Negative (ìë™ ê°ì§€)")
st.caption("ì—‘ì…€(Review.xlsx)ì˜ Sentiment, Review ì»¬ëŸ¼ìœ¼ë¡œ í•™ìŠµ â†’ ë¦¬ë·° ì…ë ¥í•˜ë©´ ë°”ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.header("1) ë°ì´í„° ë¡œë“œ")
    uploaded = st.file_uploader("Review.xlsx ì—…ë¡œë“œ", type=["xlsx"])

    st.header("2) í•™ìŠµ ì„¤ì •")
    vocab_size = st.number_input("VOCAB_SIZE (ë‹¨ì–´ ì‚¬ì „ í¬ê¸°)", 5000, 50000, 20000, step=1000)
    max_len = st.number_input("MAX_LEN (íŒ¨ë”© ê¸¸ì´)", 20, 300, 120, step=10)
    epochs = st.number_input("epochs", 1, 50, 10, step=1)
    batch_size = st.selectbox("batch_size", [8, 16, 32, 64], index=1)
    test_size = st.slider("test_size", 0.1, 0.4, 0.2, 0.05)
    random_state = st.number_input("random_state", 0, 9999, 42, step=1)

    st.header("3) ë¶ˆìš©ì–´(Stopwords)")
    st.write(f"í˜„ì¬ ë¶ˆìš©ì–´ ê°œìˆ˜: **{len(STOPWORDS)}**")
    extra_sw = st.text_input("ì¶”ê°€ ë¶ˆìš©ì–´(ì‰¼í‘œë¡œ êµ¬ë¶„)", value="")
    if extra_sw.strip():
        for w in extra_sw.split(","):
            w = w.strip()
            if w:
                STOPWORDS.add(w)

    st.divider()
    train_btn = st.button("ğŸš€ í•™ìŠµ ì‹œì‘", use_container_width=True)

# ----------------------------
# ë°ì´í„° ë¡œë“œ
# ----------------------------
df = load_review_xlsx(uploaded_file=uploaded)

if df is None:
    st.warning("Review.xlsxë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜, í”„ë¡œì íŠ¸ í´ë”ì˜ data/Review.xlsxë¥¼ í™•ì¸í•´ì¤˜.")
    st.stop()

required_cols = {"Sentiment", "Review"}
missing = required_cols - set(df.columns)
if missing:
    st.error(f"ì—‘ì…€ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ì–´: {missing}\ní˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")
    st.stop()

df = df[["Sentiment", "Review"]].copy()
df["Sentiment"] = df["Sentiment"].astype(str).fillna("").str.strip()
df["Review"] = df["Review"].astype(str).fillna("").str.strip()
df = df[(df["Sentiment"] != "") & (df["Review"] != "")].copy()

st.subheader("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
c1, c2 = st.columns([2, 1])
with c1:
    st.dataframe(df.head(10), use_container_width=True)
with c2:
    st.write("ë¼ë²¨ ë¶„í¬")
    st.write(df["Sentiment"].value_counts())

# ----------------------------
# í•™ìŠµ ì‹¤í–‰ (ì„¸ì…˜ ìºì‹œ)
# ----------------------------
if "trained" not in st.session_state:
    st.session_state.trained = False
    st.session_state.model = None
    st.session_state.tokenizer = None
    st.session_state.le = None
    st.session_state.max_len = None

def train_pipeline():
    texts_raw = df["Review"].tolist()
    labels_raw = df["Sentiment"].tolist()

    # ì „ì²˜ë¦¬
    texts = [preprocess_to_string(t) for t in texts_raw]

    # ë¼ë²¨ ì¸ì½”ë”©
    le = LabelEncoder()
    y = le.fit_transform(labels_raw)

    # split
    X_train, X_test, y_train, y_test = train_test_split(
        texts, y, test_size=float(test_size), random_state=int(random_state), stratify=y
    )

    # Tokenizer + Padding
    tokenizer = Tokenizer(num_words=int(vocab_size), oov_token="<OOV>")
    tokenizer.fit_on_texts(X_train)

    def to_pad(x_list):
        seq = tokenizer.texts_to_sequences(x_list)
        return pad_sequences(seq, maxlen=int(max_len), padding="post", truncating="post")

    X_train_pad = to_pad(X_train)
    X_test_pad = to_pad(X_test)

    num_classes = len(le.classes_)
    model = build_model(vocab_size=int(vocab_size), max_len=int(max_len), num_classes=num_classes)

    # EarlyStopping
    callbacks = [
        tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=2, restore_best_weights=True)
    ]

    history = model.fit(
        X_train_pad, y_train,
        validation_split=0.2,
        epochs=int(epochs),
        batch_size=int(batch_size),
        callbacks=callbacks,
        verbose=0
    )

    # í‰ê°€
    probs = model.predict(X_test_pad, verbose=0)
    pred = np.argmax(probs, axis=1)

    acc = accuracy_score(y_test, pred)
    report = classification_report(y_test, pred, target_names=le.classes_, digits=4)
