# =========================================
# Streamlit LSTM ê°ì •ë¶„ë¥˜ ì•±
# - ì…ë ¥ ë°ì´í„°: Review.xlsx (Sentiment, Review)
# - ë ˆí¬ êµ¬ì¡°:
#   â”œâ”€ app.py
#   â”œâ”€ Review.xlsx
#   â””â”€ requirements.txt
# =========================================

import re
import numpy as np
import pandas as pd
import streamlit as st
from pathlib import Path

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers, models

# ----------------------------
# Streamlit ê¸°ë³¸ ì„¤ì •
# ----------------------------
st.set_page_config(page_title="ë¦¬ë·° ê°ì • ë¶„ì„", layout="centered")
st.title("ğŸ“Š ë¦¬ë·° ê°ì • ë¶„ì„ (LSTM)")
st.caption("Positive / Negative ë¶„ë¥˜")

# ----------------------------
# 1) ë°ì´í„° ë¡œë“œ
# ----------------------------
BASE_DIR = Path(__file__).resolve().parent
DATA_PATH = BASE_DIR / "Review.xlsx"

if not DATA_PATH.exists():
    st.error("âŒ Review.xlsxë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

df = pd.read_excel(DATA_PATH)

required_cols = {"Sentiment", "Review"}
if not required_cols.issubset(df.columns):
    st.error(f"âŒ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {required_cols}")
    st.stop()

df = df[["Sentiment", "Review"]].dropna()
df["Review"] = df["Review"].astype(str)

st.success(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(df)} rows)")
st.write(df["Sentiment"].value_counts())

# ----------------------------
# 2) ë¶ˆìš©ì–´ & ì „ì²˜ë¦¬
# ----------------------------
stopwords = set([
    "ì´", "ê°€", "ì„", "ë¥¼", "ì€", "ëŠ”", "ì—", "ì—ì„œ", "ì—ê²Œ",
    "ì˜", "ì™€", "ê³¼", "ë„", "ë¡œ", "ìœ¼ë¡œ",
    "í•˜ë‹¤", "ë˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤",
    "ê·¸", "ì €", "ê²ƒ", "ìˆ˜",
    "ì¢€", "ì˜", "ë§¤ìš°", "ì •ë§", "ë„ˆë¬´",
    "ë•Œë¬¸", "ê°™ë‹¤"
])

def clean_text(text):
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", text)
    tokens = text.split()
    tokens = [t for t in tokens if t not in stopwords and len(t) > 1]
    return " ".join(tokens)

texts = [clean_text(t) for t in df["Review"].tolist()]
labels = df["Sentiment"].tolist()

# ----------------------------
# 3) ë¼ë²¨ ì¸ì½”ë”©
# ----------------------------
le = LabelEncoder()
y = le.fit_transform(labels)

st.write("ğŸ”– ë¼ë²¨ ë§¤í•‘:", dict(zip(le.classes_, range(len(le.classes_)))))

# ----------------------------
# 4) ë°ì´í„° ë¶„í• 
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    texts, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# 5) Tokenizer + Padding
# ----------------------------
VOCAB_SIZE = 15000
MAX_LEN = 120

tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

def encode(texts):
    seq = tokenizer.texts_to_sequences(texts)
    return pad_sequences(seq, maxlen=MAX_LEN, padding="post", truncating="post")

X_train_pad = encode(X_train)
X_test_pad = encode(X_test)

# ----------------------------
# 6) LSTM ëª¨ë¸
# ----------------------------
model = models.Sequential([
    layers.Embedding(VOCAB_SIZE, 128, input_length=MAX_LEN),
    layers.Bidirectional(layers.LSTM(64)),
    layers.Dropout(0.3),
    layers.Dense(32, activation="relu"),
    layers.Dense(len(le.classes_), activation="softmax")
])

model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# ----------------------------
# 7) í•™ìŠµ
# ----------------------------
with st.spinner("ğŸ§  ëª¨ë¸ í•™ìŠµ ì¤‘..."):
    model.fit(
        X_train_pad,
        y_train,
        epochs=6,
        batch_size=16,
        validation_split=0.1,
        verbose=0
    )

st.success("âœ… í•™ìŠµ ì™„ë£Œ!")

# ----------------------------
# 8) ë¦¬ë·° ì…ë ¥ â†’ ì˜ˆì¸¡
# ----------------------------
st.subheader("âœï¸ ë¦¬ë·° ì…ë ¥")

user_input = st.text_area(
    "ë¦¬ë·°ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
    placeholder="ì˜ˆ: ë‹´ë°° ëƒ„ìƒˆê°€ ë„ˆë¬´ ì‹¬í•´ì„œ ë¶ˆì¾Œí–ˆì–´ìš”"
)

if st.button("ì˜ˆì¸¡í•˜ê¸°"):
    if user_input.strip() == "":
        st.warning("ë¦¬ë·°ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        proc = clean_text(user_input)
        pad = encode([proc])
        probs = model.predict(pad)[0]
        idx = int(np.argmax(probs))
        label = le.inverse_transform([idx])[0]
        conf = float(np.max(probs))

        st.markdown(f"### ğŸ§¾ ì˜ˆì¸¡ ê²°ê³¼: **{label}**")
        st.progress(conf)

        st.write("ğŸ“Š í´ë˜ìŠ¤ë³„ í™•ë¥ ")
        for c, p in zip(le.classes_, probs):
            st.write(f"- {c}: {p:.3f}")

st.caption("âœ” LSTM / Tokenizer / Padding / ë¶ˆìš©ì–´ ì œê±° ì ìš© ì™„ë£Œ")
